compiled_data <- lapply(as.list(weeks), function(x) {
# Getting last week and current week data
wk <- filter(d, proj_week == x) %>% select(-prev_wk_fantasy_fdpt)
if(x < max(weeks)){
lw <- filter(d, proj_week == (x+1)) %>% select(proj_player, prev_wk_fantasy_fdpt)
} else {lw <- tw}
# Add last week data to current week
combined <- left_join(wk, lw, by = "proj_player") %>% filter(is.na(prev_wk_fantasy_fdpt) == F)
# Adding whether they are top ten or not
top <- combined %>% slice_max(order_by = prev_wk_fantasy_fdpt, n = 10) %>% mutate(top_ten = factor(1))
not_top <- combined %>% slice_min(order_by = prev_wk_fantasy_fdpt, n = nrow(combined) - 10) %>% mutate(top_ten = factor(0))
if(nrow(top) + nrow(not_top) != nrow(combined)) {print(x); break}
# Put back into one DF
combined <- rbind(top, not_top)
return(combined)
})
# Putting into a DF and doing some QB-specific processing
processed_data <- do.call(rbind, compiled_data)
qb_col <- c("proj_pos","proj_player","prev_wk_fantasy_fdpt","proj_week",
"pts_vs_g", "projected_own","proj_afpa_rk","line","total",
"implied_total","pts_vs_passing_att", "pts_vs_passing_yds", "pts_vs_passing_td",
"pts_vs_fantasy_per_game_fdpt","def_red_zone_td","def_red_zone_pct",
"def_dvoa", "def_pass_dvoa", "def_dline_pass_rank",
"def_dline_pass_adjusted_sack_rate","def_pass_qb_rating_allowed",
"def_pass_adj_net_yds_per_att", "off_oline_pass_adjusted_sack_rate",
"ytd_pass_yds_per_gm","ytd_pass_td","adv_passing_iay","pass_dyar",
"adv_passing_ontgt_per","adv_passing_bad_per",
"adv_passing_prss_per","pass_eyds","pass_yards","rush_eyds","rush_dyar","rush_yards",
"passing_twenty_att","passing_twenty_td","top_ten")
processed_data <- select(processed_data, qb_col) %>%
mutate(adv_passing_iay = round(adv_passing_iay / as.numeric(pts_vs_g), 2),
pass_dyar = round(pass_dyar/as.numeric(pts_vs_g),2),
rush_yards = round(rush_yards / as.numeric(pts_vs_g),2),
pass_yds_diff = round((pass_eyds - pass_yards)/as.numeric(pts_vs_g),2),
rush_yds_diff = round((rush_eyds - rush_yards)/as.numeric(pts_vs_g),2),
DVOA_Diff = def_pass_dvoa - def_dvoa,
pts_vs_passing_att = round(as.numeric(pts_vs_passing_att) / as.numeric(pts_vs_g),2),
pts_vs_passing_yds = round(as.numeric(pts_vs_passing_yds) / as.numeric(pts_vs_g),2),
pts_vs_passing_td = round(as.numeric(pts_vs_passing_td) / as.numeric(pts_vs_g),2),
pts_vs_fantasy_per_game_fdpt = as.numeric(pts_vs_fantasy_per_game_fdpt)) %>%
select(-pts_vs_g)
# For now, doing a median replace for NA's
# TODO - smarter NA replacement
for(ii in c(5:36,38:40)) {processed_data[,ii][is.na(processed_data[,ii])] <- median(unlist(processed_data[,ii]), na.rm = T)}
# Training PCA Model analysis -----
d_wk <- filter(processed_data, proj_week != wk_num-1)
pca <- prcomp(d_wk[,c(6:36,38:39)], scale = T, center = T)
summary(pca)
View(errors)
screeplot(pca, type = "l", npcs = 15, main = "Screeplot of the first 10 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
col=c("red"), lty=5, cex=0.6)
library(factoextra)
fviz_pca_ind(pca, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = d_wk$top_ten,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Top 10 (1 = Yes)") +
ggtitle("2D PCA-plot") +
theme(plot.title = element_text(hjust = 0.5))
fviz_pca_biplot(pca, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = d_wk$top_ten,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Top 10 (1 = Yes)") +
ggtitle("2D PCA-plot") +
theme(plot.title = element_text(hjust = 0.5))
# Highlighting top five variables on each component ----
pc1 <- abs(pca$rotation) %>% as.data.frame %>% rownames_to_column %>%
select(rowname, PC1) %>% arrange(desc(PC1)) %>% head(5)
as.vector(pc1$rowname)
pc2 <- abs(pca$rotation) %>% as.data.frame %>% rownames_to_column %>%
select(rowname, PC2) %>% arrange(desc(PC2)) %>% head(5)
as.vector(pc2$rowname)
# Working out the accuracy on the training data ----
pca_pred <- as.data.frame(predict(pca, d_wk)) %>% mutate(top_ten = d_wk$top_ten)
m <- glm(top_ten ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10, data = pca_pred, family = "binomial")
summary(m)
pca_pred$binomial_pred <- predict.glm(m, pca_pred, type = "response")
# Sensitivity Analysis for threshold -----
for(ii in c(0.30,0.40,0.45,0.50,0.55,0.60,0.65)) {
# Getting working version of df
wd <- pca_pred
# do accuracy calculation
wd$outcome_bi = ifelse(wd$binomial_pred > ii, 1, 0)
wd$tp = ifelse(wd$outcome_bi == 1 & wd$top_ten == 1,1,0)
wd$fp = ifelse(wd$outcome_bi == 1 & wd$top_ten == 0,1,0)
wd$fn = ifelse(wd$outcome_bi == 0 & wd$top_ten == 1,1,0)
wd$tn = ifelse(wd$outcome_bi == 0 & wd$top_ten == 0,1,0)
# Calculating Values
accuracy <- (sum(wd$tp) + sum(wd$tn)) / nrow(wd)
precision <- sum(wd$tp) / (sum(wd$tp) + sum(wd$fp))
recall <- sum(wd$tp) / (sum(wd$tp) + sum(wd$fn))
f1 <- 2 * ((precision * recall)/ (precision + recall))
# Print result
cat(paste0("Threshold of: ",ii,
" = Accuracy: ", round(accuracy*100,2),
" | Precision: ", round(precision*100,2),
" | Recall: ", round(recall*100,2),
" | F1: ", round(f1,2),
" \n"))
}
# Testing data
test_data <- filter(processed_data, proj_week == (wk_num-1))
pca_test <- as.data.frame(predict(pca, test_data)) %>% mutate(top_ten = test_data$top_ten)
pca_test$binomial_pred <- predict.glm(m, pca_test, type = "response")
# Calculating Accuracy
pca_test$outcome_bi = ifelse(pca_test$binomial_pred > 0.5, 1, 0)
#pca_test$accuracy = ifelse(pca_test$outcome_bi == as.numeric(as.character(pca_test$top_ten)),1,0)
pca_test$tp = ifelse(pca_test$outcome_bi == 1 & pca_test$top_ten == 1,1,0)
pca_test$fp = ifelse(pca_test$outcome_bi == 1 & pca_test$top_ten == 0,1,0)
pca_test$fn = ifelse(pca_test$outcome_bi == 0 & pca_test$top_ten == 1,1,0)
pca_test$tn = ifelse(pca_test$outcome_bi == 0 & pca_test$top_ten == 0,1,0)
# Calculating Values
(accuracy <- (sum(pca_test$tp) + sum(pca_test$tn)) / nrow(pca_test))
(precision <- sum(pca_test$tp) / (sum(pca_test$tp) + sum(pca_test$fp)))
(recall <- sum(pca_test$tp) / (sum(pca_test$tp) + sum(pca_test$fn)))
(f1 <- 2 * ((precision * recall)/ (precision + recall)))
# Calculate MSE
pca_test$sq_error <- (as.numeric(as.character(pca_test$top_ten)) - pca_test$binomial_pred)^2
mean(pca_test$sq_error)
### Sampling on a random sample of 25 - 10 top 10 and 15 not top 10
tt <- filter(processed_data, top_ten == 1)
ntt <- filter(processed_data, top_ten == 0)
runif(10, 1, nrow(tt))
sample.int(10, 1, nrow(tt))
sample.int(c(1:nrow(tt),10)
)
sample.int(1:nrow(tt),10)
sample.int(c(1:nrow(tt)),10)
sample.int(1e10, 12, replace = TRUE)
sample.int(nrow(tt), 12, replace = TRUE)
sample.int(nrow(tt), 10, replace = F)
### Sampling on a random sample of 25 - 10 top 10 and 15 not top 10
set.seed(1234)
tt_player <- sample.int(nrow(tt), 10, replace = FALSE)
ntt <- filter(processed_data, top_ten == 0)
ntt_player <- sample.int(nrow(ntt), 15, replace = FALSE)
random_tt <- rbind(tt[tt_player,], ntt[ntt_player,])
View(random_tt)
# Testing data
rand_pca_test <- as.data.frame(predict(pca, random_tt)) %>% mutate(top_ten = random_tt$top_ten)
rand_pca_test$binomial_pred <- predict.glm(m, rand_pca_test, type = "response")
# Calculating Accuracy
rand_pca_test$outcome_bi = ifelse(rand_pca_test$binomial_pred > 0.5, 1, 0)
#pca_test$accuracy = ifelse(pca_test$outcome_bi == as.numeric(as.character(pca_test$top_ten)),1,0)
rand_pca_test$tp = ifelse(rand_pca_test$outcome_bi == 1 & rand_pca_test$top_ten == 1,1,0)
rand_pca_test$fp = ifelse(rand_pca_test$outcome_bi == 1 & rand_pca_test$top_ten == 0,1,0)
rand_pca_test$fn = ifelse(rand_pca_test$outcome_bi == 0 & rand_pca_test$top_ten == 1,1,0)
rand_pca_test$tn = ifelse(rand_pca_test$outcome_bi == 0 & rand_pca_test$top_ten == 0,1,0)
# Calculating Values
(accuracy <- (sum(rand_pca_test$tp) + sum(rand_pca_test$tn)) / nrow(rand_pca_test))
(precision <- sum(rand_pca_test$tp) / (sum(rand_pca_test$tp) + sum(rand_pca_test$fp)))
(recall <- sum(rand_pca_test$tp) / (sum(rand_pca_test$tp) + sum(rand_pca_test$fn)))
(f1 <- 2 * ((precision * recall)/ (precision + recall)))
# Calculate MSE
rand_pca_test$sq_error <- (as.numeric(as.character(rand_pca_test$top_ten)) - rand_pca_test$binomial_pred)^2
mean(rand_pca_test$sq_error)
## Top 10
wk_num <- 9
d <- readxl::read_xlsx("C:/Users/mattw/Documents/ff_shiny_app/ff_app/2020_data/merged_data/2020_weeks_2to9_combined_vegas_fixes.xlsx") %>%
filter(proj_week <= wk_num - 1 & proj_week > 3 & proj_pos == "QB")
# need previous week fantasy data, so reading in this week's data to look at
tw <- readxl::read_xlsx(paste0("C:/Users/mattw/Documents/ff_shiny_app/ff_app/data/all_data_wk_",wk_num,"_2020.xlsx")) %>%
filter(proj_pos == "QB") %>%
select(proj_player, prev_wk_fantasy_fdpt)
weeks <- unique(d$proj_week)
compiled_data <- lapply(as.list(weeks), function(x) {
# Getting last week and current week data
wk <- filter(d, proj_week == x) %>% select(-prev_wk_fantasy_fdpt)
if(x < max(weeks)){
lw <- filter(d, proj_week == (x+1)) %>% select(proj_player, prev_wk_fantasy_fdpt)
} else {lw <- tw}
# Add last week data to current week
combined <- left_join(wk, lw, by = "proj_player") %>% filter(is.na(prev_wk_fantasy_fdpt) == F)
# Adding whether they are top ten or not
top <- combined %>% slice_max(order_by = prev_wk_fantasy_fdpt, n = 10) %>% mutate(top_ten = factor(1))
not_top <- combined %>% slice_min(order_by = prev_wk_fantasy_fdpt, n = nrow(combined) - 10) %>% mutate(top_ten = factor(0))
if(nrow(top) + nrow(not_top) != nrow(combined)) {print(x); break}
# Put back into one DF
combined <- rbind(top, not_top)
return(combined)
})
# Putting into a DF and doing some QB-specific processing
processed_data <- do.call(rbind, compiled_data)
qb_col <- c("proj_pos","proj_player","prev_wk_fantasy_fdpt","proj_week",
"pts_vs_g", "projected_own","proj_afpa_rk","line","total",
"implied_total","pts_vs_passing_att", "pts_vs_passing_yds", "pts_vs_passing_td",
"pts_vs_fantasy_per_game_fdpt","def_red_zone_td","def_red_zone_pct",
"def_dvoa", "def_pass_dvoa", "def_dline_pass_rank",
"def_dline_pass_adjusted_sack_rate","def_pass_qb_rating_allowed",
"def_pass_adj_net_yds_per_att", "off_oline_pass_adjusted_sack_rate",
"ytd_pass_yds_per_gm","ytd_pass_td","adv_passing_iay","pass_dyar",
"adv_passing_ontgt_per","adv_passing_bad_per",
"adv_passing_prss_per","pass_eyds","pass_yards","rush_eyds","rush_dyar","rush_yards",
"passing_twenty_att","passing_twenty_td","top_ten")
processed_data <- select(processed_data, qb_col) %>%
mutate(adv_passing_iay = round(adv_passing_iay / as.numeric(pts_vs_g), 2),
pass_dyar = round(pass_dyar/as.numeric(pts_vs_g),2),
rush_yards = round(rush_yards / as.numeric(pts_vs_g),2),
pass_yds_diff = round((pass_eyds - pass_yards)/as.numeric(pts_vs_g),2),
rush_yds_diff = round((rush_eyds - rush_yards)/as.numeric(pts_vs_g),2),
DVOA_Diff = def_pass_dvoa - def_dvoa,
pts_vs_passing_att = round(as.numeric(pts_vs_passing_att) / as.numeric(pts_vs_g),2),
pts_vs_passing_yds = round(as.numeric(pts_vs_passing_yds) / as.numeric(pts_vs_g),2),
pts_vs_passing_td = round(as.numeric(pts_vs_passing_td) / as.numeric(pts_vs_g),2),
pts_vs_fantasy_per_game_fdpt = as.numeric(pts_vs_fantasy_per_game_fdpt)) %>%
select(-pts_vs_g)
# For now, doing a median replace for NA's
# TODO - smarter NA replacement
for(ii in c(5:36,38:40)) {processed_data[,ii][is.na(processed_data[,ii])] <- median(unlist(processed_data[,ii]), na.rm = T)}
# Training PCA Model analysis -----
d_wk <- filter(processed_data, proj_week != wk_num-1)
pca <- prcomp(d_wk[,c(6:36,38:39)], scale = T, center = T)
screeplot(pca, type = "l", npcs = 15, main = "Screeplot of the first 10 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
col=c("red"), lty=5, cex=0.6)
library(factoextra)
fviz_pca_ind(pca, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = d_wk$top_ten,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Top 10 (1 = Yes)") +
ggtitle("2D PCA-plot") +
theme(plot.title = element_text(hjust = 0.5))
fviz_pca_biplot(pca, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = d_wk$top_ten,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Top 10 (1 = Yes)") +
ggtitle("2D PCA-plot") +
theme(plot.title = element_text(hjust = 0.5))
# Highlighting top five variables on each component ----
pc1 <- abs(pca$rotation) %>% as.data.frame %>% rownames_to_column %>%
select(rowname, PC1) %>% arrange(desc(PC1)) %>% head(5)
as.vector(pc1$rowname)
pc2 <- abs(pca$rotation) %>% as.data.frame %>% rownames_to_column %>%
select(rowname, PC2) %>% arrange(desc(PC2)) %>% head(5)
as.vector(pc2$rowname)
# Working out the accuracy on the training data ----
pca_pred <- as.data.frame(predict(pca, d_wk)) %>% mutate(top_ten = d_wk$top_ten)
m <- glm(top_ten ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10, data = pca_pred, family = "binomial")
summary(m)
pca_pred$binomial_pred <- predict.glm(m, pca_pred, type = "response")
# Sensitivity Analysis for threshold -----
for(ii in c(0.30,0.40,0.45,0.50,0.55,0.60,0.65)) {
# Getting working version of df
wd <- pca_pred
# do accuracy calculation
wd$outcome_bi = ifelse(wd$binomial_pred > ii, 1, 0)
wd$tp = ifelse(wd$outcome_bi == 1 & wd$top_ten == 1,1,0)
wd$fp = ifelse(wd$outcome_bi == 1 & wd$top_ten == 0,1,0)
wd$fn = ifelse(wd$outcome_bi == 0 & wd$top_ten == 1,1,0)
wd$tn = ifelse(wd$outcome_bi == 0 & wd$top_ten == 0,1,0)
# Calculating Values
accuracy <- (sum(wd$tp) + sum(wd$tn)) / nrow(wd)
precision <- sum(wd$tp) / (sum(wd$tp) + sum(wd$fp))
recall <- sum(wd$tp) / (sum(wd$tp) + sum(wd$fn))
f1 <- 2 * ((precision * recall)/ (precision + recall))
# Print result
cat(paste0("Threshold of: ",ii,
" = Accuracy: ", round(accuracy*100,2),
" | Precision: ", round(precision*100,2),
" | Recall: ", round(recall*100,2),
" | F1: ", round(f1,2),
" \n"))
}
# Testing data
test_data <- filter(processed_data, proj_week == (wk_num-1))
pca_test <- as.data.frame(predict(pca, test_data)) %>% mutate(top_ten = test_data$top_ten)
pca_test$binomial_pred <- predict.glm(m, pca_test, type = "response")
# Calculating Accuracy
pca_test$outcome_bi = ifelse(pca_test$binomial_pred > 0.5, 1, 0)
#pca_test$accuracy = ifelse(pca_test$outcome_bi == as.numeric(as.character(pca_test$top_ten)),1,0)
pca_test$tp = ifelse(pca_test$outcome_bi == 1 & pca_test$top_ten == 1,1,0)
pca_test$fp = ifelse(pca_test$outcome_bi == 1 & pca_test$top_ten == 0,1,0)
pca_test$fn = ifelse(pca_test$outcome_bi == 0 & pca_test$top_ten == 1,1,0)
pca_test$tn = ifelse(pca_test$outcome_bi == 0 & pca_test$top_ten == 0,1,0)
# Calculating Values
(accuracy <- (sum(pca_test$tp) + sum(pca_test$tn)) / nrow(pca_test))
(precision <- sum(pca_test$tp) / (sum(pca_test$tp) + sum(pca_test$fp)))
(recall <- sum(pca_test$tp) / (sum(pca_test$tp) + sum(pca_test$fn)))
(f1 <- 2 * ((precision * recall)/ (precision + recall)))
# Calculate MSE
pca_test$sq_error <- (as.numeric(as.character(pca_test$top_ten)) - pca_test$binomial_pred)^2
mean(pca_test$sq_error)
# Creating a plot to see where we missed worst
errors <- cbind(test_data[,2], pca_test[,c(34,35,41)])
ggplot(data = errors, aes(x = binomial_pred, y = sq_error)) +
geom_point(shape = 21, size = 6, aes(fill = top_ten)) +
geom_label(label = errors$proj_player,
nudge_x = 0.01, nudge_y = 0.01) +
theme_bw() + ggtitle("Week 9; MSE = 0.38")
### Sampling on a random sample of 25 - 10 top 10 and 15 not top 10
set.seed(1234)
tt <- filter(processed_data, top_ten == 1)
tt_player <- sample.int(nrow(tt), 10, replace = FALSE)
ntt <- filter(processed_data, top_ten == 0)
ntt_player <- sample.int(nrow(ntt), 15, replace = FALSE)
random_tt <- rbind(tt[tt_player,], ntt[ntt_player,])
# Testing data
rand_pca_test <- as.data.frame(predict(pca, random_tt)) %>% mutate(top_ten = random_tt$top_ten)
rand_pca_test$binomial_pred <- predict.glm(m, rand_pca_test, type = "response")
# Calculating Accuracy
rand_pca_test$outcome_bi = ifelse(rand_pca_test$binomial_pred > 0.5, 1, 0)
#pca_test$accuracy = ifelse(pca_test$outcome_bi == as.numeric(as.character(pca_test$top_ten)),1,0)
rand_pca_test$tp = ifelse(rand_pca_test$outcome_bi == 1 & rand_pca_test$top_ten == 1,1,0)
rand_pca_test$fp = ifelse(rand_pca_test$outcome_bi == 1 & rand_pca_test$top_ten == 0,1,0)
rand_pca_test$fn = ifelse(rand_pca_test$outcome_bi == 0 & rand_pca_test$top_ten == 1,1,0)
rand_pca_test$tn = ifelse(rand_pca_test$outcome_bi == 0 & rand_pca_test$top_ten == 0,1,0)
# Calculating Values
(accuracy <- (sum(rand_pca_test$tp) + sum(rand_pca_test$tn)) / nrow(rand_pca_test))
(precision <- sum(rand_pca_test$tp) / (sum(rand_pca_test$tp) + sum(rand_pca_test$fp)))
(recall <- sum(rand_pca_test$tp) / (sum(rand_pca_test$tp) + sum(rand_pca_test$fn)))
(f1 <- 2 * ((precision * recall)/ (precision + recall)))
# Calculate MSE
rand_pca_test$sq_error <- (as.numeric(as.character(rand_pca_test$top_ten)) - rand_pca_test$binomial_pred)^2
mean(rand_pca_test$sq_error)
## Top 10
wk_num <- 10
d <- readxl::read_xlsx("C:/Users/mattw/Documents/ff_shiny_app/ff_app/2020_data/merged_data/2020_weeks_2to9_combined_vegas_fixes.xlsx") %>%
filter(proj_week <= wk_num - 1 & proj_week > 3 & proj_pos == "QB")
# need previous week fantasy data, so reading in this week's data to look at
tw <- readxl::read_xlsx(paste0("C:/Users/mattw/Documents/ff_shiny_app/ff_app/data/all_data_wk_",wk_num,"_2020.xlsx")) %>%
filter(proj_pos == "QB") %>%
select(proj_player, prev_wk_fantasy_fdpt)
weeks <- unique(d$proj_week)
compiled_data <- lapply(as.list(weeks), function(x) {
# Getting last week and current week data
wk <- filter(d, proj_week == x) %>% select(-prev_wk_fantasy_fdpt)
if(x < max(weeks)){
lw <- filter(d, proj_week == (x+1)) %>% select(proj_player, prev_wk_fantasy_fdpt)
} else {lw <- tw}
# Add last week data to current week
combined <- left_join(wk, lw, by = "proj_player") %>% filter(is.na(prev_wk_fantasy_fdpt) == F)
# Adding whether they are top ten or not
top <- combined %>% slice_max(order_by = prev_wk_fantasy_fdpt, n = 10) %>% mutate(top_ten = factor(1))
not_top <- combined %>% slice_min(order_by = prev_wk_fantasy_fdpt, n = nrow(combined) - 10) %>% mutate(top_ten = factor(0))
if(nrow(top) + nrow(not_top) != nrow(combined)) {print(x); break}
# Put back into one DF
combined <- rbind(top, not_top)
return(combined)
})
# Putting into a DF and doing some QB-specific processing
processed_data <- do.call(rbind, compiled_data)
qb_col <- c("proj_pos","proj_player","prev_wk_fantasy_fdpt","proj_week",
"pts_vs_g", "projected_own","proj_afpa_rk","line","total",
"implied_total","pts_vs_passing_att", "pts_vs_passing_yds", "pts_vs_passing_td",
"pts_vs_fantasy_per_game_fdpt","def_red_zone_td","def_red_zone_pct",
"def_dvoa", "def_pass_dvoa", "def_dline_pass_rank",
"def_dline_pass_adjusted_sack_rate","def_pass_qb_rating_allowed",
"def_pass_adj_net_yds_per_att", "off_oline_pass_adjusted_sack_rate",
"ytd_pass_yds_per_gm","ytd_pass_td","adv_passing_iay","pass_dyar",
"adv_passing_ontgt_per","adv_passing_bad_per",
"adv_passing_prss_per","pass_eyds","pass_yards","rush_eyds","rush_dyar","rush_yards",
"passing_twenty_att","passing_twenty_td","top_ten")
processed_data <- select(processed_data, qb_col) %>%
mutate(adv_passing_iay = round(adv_passing_iay / as.numeric(pts_vs_g), 2),
pass_dyar = round(pass_dyar/as.numeric(pts_vs_g),2),
rush_yards = round(rush_yards / as.numeric(pts_vs_g),2),
pass_yds_diff = round((pass_eyds - pass_yards)/as.numeric(pts_vs_g),2),
rush_yds_diff = round((rush_eyds - rush_yards)/as.numeric(pts_vs_g),2),
DVOA_Diff = def_pass_dvoa - def_dvoa,
pts_vs_passing_att = round(as.numeric(pts_vs_passing_att) / as.numeric(pts_vs_g),2),
pts_vs_passing_yds = round(as.numeric(pts_vs_passing_yds) / as.numeric(pts_vs_g),2),
pts_vs_passing_td = round(as.numeric(pts_vs_passing_td) / as.numeric(pts_vs_g),2),
pts_vs_fantasy_per_game_fdpt = as.numeric(pts_vs_fantasy_per_game_fdpt)) %>%
select(-pts_vs_g)
# For now, doing a median replace for NA's
# TODO - smarter NA replacement
for(ii in c(5:36,38:40)) {processed_data[,ii][is.na(processed_data[,ii])] <- median(unlist(processed_data[,ii]), na.rm = T)}
# Training PCA Model analysis -----
d_wk <- filter(processed_data, proj_week != wk_num-1)
pca <- prcomp(d_wk[,c(6:36,38:39)], scale = T, center = T)
screeplot(pca, type = "l", npcs = 15, main = "Screeplot of the first 10 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
col=c("red"), lty=5, cex=0.6)
library(factoextra)
fviz_pca_ind(pca, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = d_wk$top_ten,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Top 10 (1 = Yes)") +
ggtitle("2D PCA-plot") +
theme(plot.title = element_text(hjust = 0.5))
fviz_pca_biplot(pca, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = d_wk$top_ten,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Top 10 (1 = Yes)") +
ggtitle("2D PCA-plot") +
theme(plot.title = element_text(hjust = 0.5))
# Highlighting top five variables on each component ----
pc1 <- abs(pca$rotation) %>% as.data.frame %>% rownames_to_column %>%
select(rowname, PC1) %>% arrange(desc(PC1)) %>% head(5)
as.vector(pc1$rowname)
pc2 <- abs(pca$rotation) %>% as.data.frame %>% rownames_to_column %>%
select(rowname, PC2) %>% arrange(desc(PC2)) %>% head(5)
as.vector(pc2$rowname)
# Working out the accuracy on the training data ----
pca_pred <- as.data.frame(predict(pca, d_wk)) %>% mutate(top_ten = d_wk$top_ten)
m <- glm(top_ten ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10, data = pca_pred, family = "binomial")
summary(m)
pca_pred$binomial_pred <- predict.glm(m, pca_pred, type = "response")
# Sensitivity Analysis for threshold -----
for(ii in c(0.30,0.40,0.45,0.50,0.55,0.60,0.65)) {
# Getting working version of df
wd <- pca_pred
# do accuracy calculation
wd$outcome_bi = ifelse(wd$binomial_pred > ii, 1, 0)
wd$tp = ifelse(wd$outcome_bi == 1 & wd$top_ten == 1,1,0)
wd$fp = ifelse(wd$outcome_bi == 1 & wd$top_ten == 0,1,0)
wd$fn = ifelse(wd$outcome_bi == 0 & wd$top_ten == 1,1,0)
wd$tn = ifelse(wd$outcome_bi == 0 & wd$top_ten == 0,1,0)
# Calculating Values
accuracy <- (sum(wd$tp) + sum(wd$tn)) / nrow(wd)
precision <- sum(wd$tp) / (sum(wd$tp) + sum(wd$fp))
recall <- sum(wd$tp) / (sum(wd$tp) + sum(wd$fn))
f1 <- 2 * ((precision * recall)/ (precision + recall))
# Print result
cat(paste0("Threshold of: ",ii,
" = Accuracy: ", round(accuracy*100,2),
" | Precision: ", round(precision*100,2),
" | Recall: ", round(recall*100,2),
" | F1: ", round(f1,2),
" \n"))
}
# Testing data
test_data <- filter(processed_data, proj_week == (wk_num-1))
pca_test <- as.data.frame(predict(pca, test_data)) %>% mutate(top_ten = test_data$top_ten)
pca_test$binomial_pred <- predict.glm(m, pca_test, type = "response")
# Calculating Accuracy
pca_test$outcome_bi = ifelse(pca_test$binomial_pred > 0.5, 1, 0)
#pca_test$accuracy = ifelse(pca_test$outcome_bi == as.numeric(as.character(pca_test$top_ten)),1,0)
pca_test$tp = ifelse(pca_test$outcome_bi == 1 & pca_test$top_ten == 1,1,0)
pca_test$fp = ifelse(pca_test$outcome_bi == 1 & pca_test$top_ten == 0,1,0)
pca_test$fn = ifelse(pca_test$outcome_bi == 0 & pca_test$top_ten == 1,1,0)
pca_test$tn = ifelse(pca_test$outcome_bi == 0 & pca_test$top_ten == 0,1,0)
# Calculating Values
(accuracy <- (sum(pca_test$tp) + sum(pca_test$tn)) / nrow(pca_test))
(precision <- sum(pca_test$tp) / (sum(pca_test$tp) + sum(pca_test$fp)))
(recall <- sum(pca_test$tp) / (sum(pca_test$tp) + sum(pca_test$fn)))
(f1 <- 2 * ((precision * recall)/ (precision + recall)))
# Calculate MSE
pca_test$sq_error <- (as.numeric(as.character(pca_test$top_ten)) - pca_test$binomial_pred)^2
mean(pca_test$sq_error)
# Creating a plot to see where we missed worst
errors <- cbind(test_data[,2], pca_test[,c(34,35,41)])
ggplot(data = errors, aes(x = binomial_pred, y = sq_error)) +
geom_point(shape = 21, size = 6, aes(fill = top_ten)) +
geom_label(label = errors$proj_player,
nudge_x = 0.01, nudge_y = 0.01) +
theme_bw() + ggtitle("Week 9; MSE = 0.38")
### Sampling on a random sample of 25 - 10 top 10 and 15 not top 10
set.seed(1234)
tt <- filter(processed_data, top_ten == 1)
tt_player <- sample.int(nrow(tt), 10, replace = FALSE)
ntt <- filter(processed_data, top_ten == 0)
ntt_player <- sample.int(nrow(ntt), 15, replace = FALSE)
random_tt <- rbind(tt[tt_player,], ntt[ntt_player,])
# Testing data
rand_pca_test <- as.data.frame(predict(pca, random_tt)) %>% mutate(top_ten = random_tt$top_ten)
rand_pca_test$binomial_pred <- predict.glm(m, rand_pca_test, type = "response")
# Calculating Accuracy
rand_pca_test$outcome_bi = ifelse(rand_pca_test$binomial_pred > 0.5, 1, 0)
#pca_test$accuracy = ifelse(pca_test$outcome_bi == as.numeric(as.character(pca_test$top_ten)),1,0)
rand_pca_test$tp = ifelse(rand_pca_test$outcome_bi == 1 & rand_pca_test$top_ten == 1,1,0)
rand_pca_test$fp = ifelse(rand_pca_test$outcome_bi == 1 & rand_pca_test$top_ten == 0,1,0)
rand_pca_test$fn = ifelse(rand_pca_test$outcome_bi == 0 & rand_pca_test$top_ten == 1,1,0)
rand_pca_test$tn = ifelse(rand_pca_test$outcome_bi == 0 & rand_pca_test$top_ten == 0,1,0)
# Calculating Values
(accuracy <- (sum(rand_pca_test$tp) + sum(rand_pca_test$tn)) / nrow(rand_pca_test))
(precision <- sum(rand_pca_test$tp) / (sum(rand_pca_test$tp) + sum(rand_pca_test$fp)))
(recall <- sum(rand_pca_test$tp) / (sum(rand_pca_test$tp) + sum(rand_pca_test$fn)))
(f1 <- 2 * ((precision * recall)/ (precision + recall)))
# Calculate MSE
rand_pca_test$sq_error <- (as.numeric(as.character(rand_pca_test$top_ten)) - rand_pca_test$binomial_pred)^2
mean(rand_pca_test$sq_error)
10/25
